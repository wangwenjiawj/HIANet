# import torch.nn as nn
# from torch.nn import functional as F
# import torch.utils.model_zoo as model_zoo
# import torch
# from torch.autograd import Variable
#
# from networks.non_local import NONLocalBlock2D
# from networks.simple_pose_net_deconv import get_pose_net
# #from networks.vit import image_net
#
# #from ipdb import set_trace
# import numpy as np
# import math
# import cv2
#
# affine_par = True
# import functools
#
# import sys, os
#
# from libs import InPlaceABN, InPlaceABNSync
# BatchNorm2d = functools.partial(InPlaceABNSync, activation='none')
# InPlaceABN = InPlaceABNSync = BatchNorm2d = nn.BatchNorm2d
#
# def conv3x3(in_planes, out_planes, stride=1):
#     "3x3 convolution with padding"
#     return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
#                      padding=1, bias=False)
#
# class Bottleneck(nn.Module):
#     expansion = 4
#     def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None, fist_dilation=1, multi_grid=1):
#         super(Bottleneck, self).__init__()
#         self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
#         self.bn1 = nn.BatchNorm2d(planes)
#         self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
#                                padding=dilation*multi_grid, dilation=dilation*multi_grid, bias=False)
#         self.bn2 = nn.BatchNorm2d(planes)
#         self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)
#         self.bn3 = nn.BatchNorm2d(planes * 4)
#         self.relu = nn.ReLU(inplace=False)
#         self.relu_inplace = nn.ReLU(inplace=True)
#         self.downsample = downsample
#         self.dilation = dilation
#         self.stride = stride
#
#     def forward(self, x):
#         residual = x
#
#         out = self.conv1(x)
#         out = self.bn1(out)
#         out = self.relu(out)
#
#         out = self.conv2(out)
#         out = self.bn2(out)
#         out = self.relu(out)
#
#         out = self.conv3(out)
#         out = self.bn3(out)
#
#         if self.downsample is not None:
#             residual = self.downsample(x)
#
#         out = out + residual
#         out = self.relu_inplace(out)
#
#         return out
#
# class ASPPModule(nn.Module):
#     """
#     Reference:
#         Chen, Liang-Chieh, et al. *"Rethinking Atrous Convolution for Semantic Image Segmentation."*
#     """
#     def __init__(self, features, inner_features=256, out_features=512, dilations=(12, 24, 36)):
#         super(ASPPModule, self).__init__()
#
#         self.conv1 = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)),
#                                    nn.Conv2d(features, inner_features, kernel_size=1, padding=0, dilation=1, bias=False),
#                                    InPlaceABNSync(inner_features)
#                                    #nn.BatchNorm2d(inner_features)
#                                    )
#         self.conv2 = nn.Sequential(nn.Conv2d(features, inner_features, kernel_size=1, padding=0, dilation=1, bias=False),
#                                    InPlaceABNSync(inner_features)
#                                    #nn.BatchNorm2d(inner_features)
#                                    )
#         self.conv3 = nn.Sequential(nn.Conv2d(features, inner_features, kernel_size=3, padding=dilations[0], dilation=dilations[0], bias=False),
#                                    InPlaceABNSync(inner_features)
#                                    #nn.BatchNorm2d(inner_features)
#                                    )
#         self.conv4 = nn.Sequential(nn.Conv2d(features, inner_features, kernel_size=3, padding=dilations[1], dilation=dilations[1], bias=False),
#                                    InPlaceABNSync(inner_features)
#                                    #nn.BatchNorm2d(inner_features)
#                                    )
#         self.conv5 = nn.Sequential(nn.Conv2d(features, inner_features, kernel_size=3, padding=dilations[2], dilation=dilations[2], bias=False),
#                                    InPlaceABNSync(inner_features)
#                                    #nn.BatchNorm2d(inner_features)
#                                    )
#
#         self.bottleneck = nn.Sequential(
#             nn.Conv2d(inner_features * 5, out_features, kernel_size=1, padding=0, dilation=1, bias=False),
#             InPlaceABNSync(out_features),
#             #nn.BatchNorm2d(out_features),
#             nn.Dropout2d(0.1)
#             )
#
#     def forward(self, x):
#
#         _, _, h, w = x.size()
#
#         feat1 = F.interpolate(self.conv1(x), size=(h, w), mode='bilinear', align_corners=True)
#
#         feat2 = self.conv2(x)
#         feat3 = self.conv3(x)
#         feat4 = self.conv4(x)
#         feat5 = self.conv5(x)
#         out = torch.cat((feat1, feat2, feat3, feat4, feat5), 1)
#
#         bottle = self.bottleneck(out)
#         return bottle
#
# class Edge_Module(nn.Module):
#
#     def __init__(self,in_fea=[256,512,1024], mid_fea=256, out_fea=2):
#         super(Edge_Module, self).__init__()
#
#         self.conv1 =  nn.Sequential(
#             nn.Conv2d(in_fea[0], mid_fea, kernel_size=1, padding=0, dilation=1, bias=False),
#             InPlaceABNSync(mid_fea)
#             #nn.BatchNorm2d(mid_fea)
#             )
#         self.conv2 =  nn.Sequential(
#             nn.Conv2d(in_fea[1], mid_fea, kernel_size=1, padding=0, dilation=1, bias=False),
#             InPlaceABNSync(mid_fea)
#             #nn.BatchNorm2d(mid_fea)
#             )
#         self.conv3 =  nn.Sequential(
#             nn.Conv2d(in_fea[2], mid_fea, kernel_size=1, padding=0, dilation=1, bias=False),
#             InPlaceABNSync(mid_fea)
#             #nn.BatchNorm2d(mid_fea)
#         )
#         self.conv4 = nn.Conv2d(mid_fea,out_fea, kernel_size=3, padding=1, dilation=1, bias=True)
#         #  self.conv5 = nn.Conv2d(out_fea*3,out_fea, kernel_size=1, padding=0, dilation=1, bias=True)
#         self.conv5 = nn.Conv2d(mid_fea*2, out_fea, kernel_size=1, padding=0, dilation=1, bias=True)
#         self.conv6 = nn.Conv2d(mid_fea*3, mid_fea*2, kernel_size=1, padding=0, bias=False)
#
#
#     def forward(self, x1, x2, x3):
#         _, _, h, w = x1.size()
#
#         edge1_fea = self.conv1(x1)
#         edge1 = self.conv4(edge1_fea)
#         edge2_fea = self.conv2(x2)
#         edge2 = self.conv4(edge2_fea)
#         edge3_fea = self.conv3(x3)
#         edge3 = self.conv4(edge3_fea)
#
#         edge2_fea =F.interpolate(edge2_fea, size=(h, w), mode='bilinear',align_corners=True)
#         edge3_fea =F.interpolate(edge3_fea, size=(h, w), mode='bilinear',align_corners=True)
#         edge2 =F.interpolate(edge2, size=(h, w), mode='bilinear',align_corners=True)
#         edge3 =F.interpolate(edge3, size=(h, w), mode='bilinear',align_corners=True)
#
#         edge = torch.cat([edge1, edge2, edge3], dim=1)
#         edge_fea = torch.cat([edge1_fea, edge2_fea, edge3_fea], dim=1)
#         edge_fea = self.conv6(edge_fea)
#         edge = self.conv5(edge_fea)
#
#         return edge, edge_fea
#
# class PSPModule(nn.Module):
#     """
#     Reference:
#         Zhao, Hengshuang, et al. *"Pyramid scene parsing network."*
#     """
#     def __init__(self, features, out_features=512, sizes=(1, 2, 3, 6)):
#         super(PSPModule, self).__init__()
#
#         self.stages = []
#         self.stages = nn.ModuleList([self._make_stage(features, out_features, size) for size in sizes])
#         self.bottleneck = nn.Sequential(
#             nn.Conv2d(features+len(sizes)*out_features, out_features, kernel_size=3, padding=1, dilation=1, bias=False),
#             #InPlaceABNSync(out_features),
#             nn.BatchNorm2d(out_features),
#             )
#
#     def _make_stage(self, features, out_features, size):
#         prior = nn.AdaptiveAvgPool2d(output_size=(size, size))
#         conv = nn.Conv2d(features, out_features, kernel_size=1, bias=False)
#         #bn = InPlaceABNSync(out_features)
#         bn = nn.BatchNorm2d(out_features)
#         return nn.Sequential(prior, conv, bn)
#         #  return nn.Sequential(prior, conv)
#
#     def forward(self, feats):
#         h, w = feats.size(2), feats.size(3)
#         priors = [ F.interpolate(input=stage(feats), size=(h, w), mode='bilinear', align_corners=True) for stage in self.stages] + [feats]
#         bottle = self.bottleneck(torch.cat(priors, 1))
#         return bottle
#
# class Decoder_Module(nn.Module):
#
#     def __init__(self, num_classes):
#         super(Decoder_Module, self).__init__()
#         self.conv1 = nn.Sequential(
#             nn.Conv2d(512, 256, kernel_size=1, padding=0, dilation=1, bias=False),
#             InPlaceABNSync(256)
#             #nn.BatchNorm2d(256)
#             )
#         self.conv2 = nn.Sequential(
#             nn.Conv2d(256, 48, kernel_size=1, stride=1, padding=0, dilation=1, bias=False),
#             InPlaceABNSync(48)
#             #nn.BatchNorm2d(48)
#             )
#         self.conv3 = nn.Sequential(
#             nn.Conv2d(768, 512, kernel_size=1, padding=0, dilation=1, bias=False),
#             InPlaceABNSync(512)
#             #nn.BatchNorm2d(512)
#             )
#         #  self.conv3 = nn.Sequential(
#                 #  nn.Conv2d(304, 256, kernel_size=3, padding=1, dilation=1, bias=False),
#                 #  InPlaceABNSync(256),
#                 #  nn.Conv2d(256, 512, kernel_size=3, padding=1, dilation=1, bias=False),
#                 #  InPlaceABNSync(512))
#         self.conv4 = nn.Conv2d(512, num_classes, kernel_size=1, padding=0, dilation=1, bias=True)
#
#     def forward(self, xt, xl):
#         _, _, h, w = xl.size()
#
#         xt = F.interpolate(xt, size=(h, w), mode='bilinear')
#         x = torch.cat([xt, xl], dim=1)
#         x = self.conv3(x)
#         seg = self.conv4(x)
#         return seg, x
#
#
# class ResNet(nn.Module):
#     def __init__(self, block, layers, num_classes, npoints):
#         self.inplanes = 128
#         super(ResNet, self).__init__()
#
#         self.conv1 = conv3x3(3, 64, stride=2)
#         self.bn1 = nn.BatchNorm2d(64)##############################no nn
#         self.relu1 = nn.ReLU(inplace=False)
#         self.conv2 = conv3x3(64, 64)
#         self.bn2 = nn.BatchNorm2d(64)
#         self.relu2 = nn.ReLU(inplace=False)
#         self.conv3 = conv3x3(64, 128)
#         self.bn3 = nn.BatchNorm2d(128)
#         self.relu3 = nn.ReLU(inplace=False)
#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
#
#         self.layer1 = self._make_layer(block, 64, layers[0])
#         self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
#         self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
#         self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=2, multi_grid=(1,1,1))
#         self.layer5 = ASPPModule(features=2048, inner_features=256, out_features=512)
#
#         self.module_list = []
#         self.edge_layer = Edge_Module()
#         self.pose_layer = get_pose_net(npoints, is_train=True)
#         self.layer6 = Decoder_Module(num_classes)
#
#         self.module_list.append(self.edge_layer)
#         self.module_list.append(self.pose_layer)
#
#         self.out_layer = nn.Conv2d(512, num_classes, kernel_size=1, padding=0, bias=True)
#
#         self.conv_fuse = nn.Sequential(
#                                       nn.Conv2d(1536, 512, kernel_size=1, padding=0, bias=False),
#                                       #InPlaceABNSync(512)
#                                       nn.BatchNorm2d(512)
#                                       )
#         self.nl_SEP = NONLocalBlock2D(in_channels=512, sub_sample=True)
#         #self.image_net = image_net()
#
#     def _make_layer(self, block, planes, blocks, stride=1, dilation=1, multi_grid=1):
#         downsample = None
#         if stride != 1 or self.inplanes != planes * block.expansion:
#             downsample = nn.Sequential(
#                 nn.Conv2d(self.inplanes, planes * block.expansion,
#                           kernel_size=1, stride=stride, bias=False),
#                 nn.BatchNorm2d(planes * block.expansion, affine=affine_par))
#
#         layers = []
#         generate_multi_grid = lambda index, grids: grids[index % len(grids)] if isinstance(grids, tuple) else 1
#         layers.append(block(self.inplanes, planes, stride, dilation=dilation, downsample=downsample, multi_grid=generate_multi_grid(0, multi_grid)))
#         self.inplanes = planes * block.expansion
#         for i in range(1, blocks):
#             layers.append(block(self.inplanes, planes, dilation=dilation, multi_grid=generate_multi_grid(i, multi_grid)))
#
#         return nn.Sequential(*layers)
#
#
#     def forward(self, x):#(1,3,256,256)
#         x = self.relu1(self.bn1(self.conv1(x)))#(1,64,128,128)
#         x = self.relu2(self.bn2(self.conv2(x)))#(1,64,128,128)
#         x = self.relu3(self.bn3(self.conv3(x)))#(1,128,128,128)
#         x = self.maxpool(x)#(1,128,64,64)
#         x2 = self.layer1(x)#(1,256,64,64)
#         x3 = self.layer2(x2)#(1,512,32,32)
#         x4 = self.layer3(x3)#(1,1024,32,32)
#         x5 = self.layer4(x4)#(1,2048,32,32)
#         x = self.layer5(x5)#(1,512,16,16)
#         edge1, edge_fea = self.edge_layer(x2, x3, x4)#edge1(1,2,64,64) edge_fea(1,512,64,64)
#         pose1, pose_fea = self.pose_layer(x5)#(1,16,64,64) pose_fea(1,512,64,64)
#         seg1, x = self.layer6(x, x2)#seg1(1,20,64,64) x(1,512,64,64)
#
#         h, w = x.size(2), x.size(3)
#         SEP_fea = self.conv_fuse(torch.cat((x, edge_fea, pose_fea), dim=1))#(1,512,64,64)
#         SEP_seg_correlation = self.nl_SEP(x, SEP_fea)#(1,512,64,64)
#         seg2 = self.out_layer(SEP_seg_correlation)#tensor(1,20,64,64)
#         return [[seg1, seg2], [edge1], [pose1]]
#
#
#         # SEP_fea = self.conv_fuse(torch.cat((x, edge_fea, pose_fea), dim=1))#(1,512,64,64)
#         # seg2 = self.image_net(SEP_fea)
#         # return seg2
#
#
# def CorrPM_Model(num_classes=20, npoints=16):
#     model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes, npoints)
#     return model
#


import torch.nn as nn
from torch.nn import functional as F
import torch.utils.model_zoo as model_zoo
import torch
from torch.autograd import Variable

#from networks.non_local import NONLocalBlock2D
from networks.change_non_local import NONLocalBlock2D
#from Attention.ImageDepthNet import ImageDepthNet
from networks.simple_pose_net_deconv import get_pose_net
from HigherHRNet.lib.models.pose_higher_hrnet import get_hrnet_pose
from HigherHRNet.lib.config import cfg
from networks.skeleton_feat import genSkeletons
#import libs.transforms as translib
#from networks.Nlocal import _NonLocalBlockND
from utile.utile import hiftmodule

#from ipdb import set_trace
import numpy as np
import math
import cv2
from einops import rearrange, repeat

affine_par = True
import functools

import sys, os

from libs import InPlaceABN, InPlaceABNSync
BatchNorm2d = functools.partial(InPlaceABNSync, activation='none')
InPlaceABN = InPlaceABNSync = BatchNorm2d = nn.BatchNorm2d


def conv3x3(in_planes, out_planes, stride=1):
    "3x3 convolution with padding"
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=1, bias=False)

class Bottleneck(nn.Module):
    expansion = 4
    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None, fist_dilation=1, multi_grid=1):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)
        self.bn1 = BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,
                               padding=dilation*multi_grid, dilation=dilation*multi_grid, bias=False)
        self.bn2 = BatchNorm2d(planes)
        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)
        self.bn3 = BatchNorm2d(planes * 4)
        self.relu = nn.ReLU(inplace=False)
        self.relu_inplace = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.dilation = dilation
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out = out + residual
        out = self.relu_inplace(out)

        return out

class ASPPModule(nn.Module):
    """
    Reference:
        Chen, Liang-Chieh, et al. *"Rethinking Atrous Convolution for Semantic Image Segmentation."*
    """
    def __init__(self, features, inner_features=256, out_features=512, dilations=(12, 24, 36)):
        super(ASPPModule, self).__init__()

        self.conv1 = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)),
                                   nn.Conv2d(features, inner_features, kernel_size=1, padding=0, dilation=1, bias=False),
                                   InPlaceABNSync(inner_features))
        self.conv2 = nn.Sequential(nn.Conv2d(features, inner_features, kernel_size=1, padding=0, dilation=1, bias=False),
                                   InPlaceABNSync(inner_features))
        self.conv3 = nn.Sequential(nn.Conv2d(features, inner_features, kernel_size=3, padding=dilations[0], dilation=dilations[0], bias=False),
                                   InPlaceABNSync(inner_features))
        self.conv4 = nn.Sequential(nn.Conv2d(features, inner_features, kernel_size=3, padding=dilations[1], dilation=dilations[1], bias=False),
                                   InPlaceABNSync(inner_features))
        self.conv5 = nn.Sequential(nn.Conv2d(features, inner_features, kernel_size=3, padding=dilations[2], dilation=dilations[2], bias=False),
                                   InPlaceABNSync(inner_features))
        self.hiftmodule = hiftmodule(cfg)

        self.bottleneck = nn.Sequential(
            nn.Conv2d(inner_features * 5, out_features, kernel_size=1, padding=0, dilation=1, bias=False),
            InPlaceABNSync(out_features),
            nn.Dropout2d(0.1)
            )

    def forward(self, x):

        _, _, h, w = x.size()

        feat1 = F.interpolate(self.conv1(x), size=(h, w), mode='bilinear', align_corners=True)#(2,256,32,32)

        feat2 = self.conv2(x)#(2,256,32,,32)
        feat3 = self.conv3(x)#(2,256,32,,32)
        feat4 = self.conv4(x)#(2,256,32,,32)
        feat5 = self.conv5(x)#(2,256,32,,32)
        # a = torch.cat((feat1, feat2), 1)#(2,512,32,,32)
        # c = torch.cat((feat2, feat3), 1)#(2,512,32,,32)
        # b = torch.cat((feat4, feat5), 1)#(2,512,32,,32)
        #out = self.hiftmodule(a, c, b)
        out = torch.cat((feat1, feat2, feat3, feat4, feat5), 1)

        bottle = self.bottleneck(out)
        return bottle

class Edge_Module(nn.Module):

    def __init__(self,in_fea=[256,512,1024], mid_fea=256, out_fea=2):
        super(Edge_Module, self).__init__()

        self.conv1 =  nn.Sequential(
            nn.Conv2d(in_fea[0], mid_fea, kernel_size=1, padding=0, dilation=1, bias=False),
            InPlaceABNSync(mid_fea)
            )
        self.conv2 =  nn.Sequential(
            nn.Conv2d(in_fea[1], mid_fea, kernel_size=1, padding=0, dilation=1, bias=False),
            InPlaceABNSync(mid_fea)
            )
        self.conv3 =  nn.Sequential(
            nn.Conv2d(in_fea[2], mid_fea, kernel_size=1, padding=0, dilation=1, bias=False),
            InPlaceABNSync(mid_fea)
        )
        self.conv4 = nn.Conv2d(mid_fea, out_fea, kernel_size=3, padding=1, dilation=1, bias=True)
        #  self.conv5 = nn.Conv2d(out_fea*3,out_fea, kernel_size=1, padding=0, dilation=1, bias=True)
        self.conv5 = nn.Conv2d(mid_fea*2, out_fea, kernel_size=1, padding=0, dilation=1, bias=True)
        self.conv6 = nn.Conv2d(mid_fea*3, mid_fea*2, kernel_size=1, padding=0, bias=False)


    def forward(self, x1, x2, x3):
        _, _, h, w = x1.size()

        edge1_fea = self.conv1(x1)
        edge1 = self.conv4(edge1_fea)
        edge2_fea = self.conv2(x2)
        edge2 = self.conv4(edge2_fea)
        edge3_fea = self.conv3(x3)
        edge3 = self.conv4(edge3_fea)

        edge2_fea =  F.interpolate(edge2_fea, size=(h, w), mode='bilinear',align_corners=True)
        edge3_fea =  F.interpolate(edge3_fea, size=(h, w), mode='bilinear',align_corners=True)
        edge2 =  F.interpolate(edge2, size=(h, w), mode='bilinear',align_corners=True)
        edge3 =  F.interpolate(edge3, size=(h, w), mode='bilinear',align_corners=True)

        edge = torch.cat([edge1, edge2, edge3], dim=1)
        edge_fea = torch.cat([edge1_fea, edge2_fea, edge3_fea], dim=1)
        edge_fea = self.conv6(edge_fea)
        edge = self.conv5(edge_fea)

        return edge, edge_fea

class PSPModule(nn.Module):
    """
    Reference:
        Zhao, Hengshuang, et al. *"Pyramid scene parsing network."*
    """
    def __init__(self, features, out_features=512, sizes=(1, 2, 3, 6)):
        super(PSPModule, self).__init__()

        self.stages = []
        self.stages = nn.ModuleList([self._make_stage(features, out_features, size) for size in sizes])
        self.bottleneck = nn.Sequential(
            nn.Conv2d(features+len(sizes)*out_features, out_features, kernel_size=3, padding=1, dilation=1, bias=False),
            InPlaceABNSync(out_features),
            )

    def _make_stage(self, features, out_features, size):
        prior = nn.AdaptiveAvgPool2d(output_size=(size, size))
        conv = nn.Conv2d(features, out_features, kernel_size=1, bias=False)
        bn = InPlaceABNSync(out_features)
        return nn.Sequential(prior, conv, bn)
        #  return nn.Sequential(prior, conv)

    def forward(self, feats):
        h, w = feats.size(2), feats.size(3)
        priors = [F.interpolate(input=stage(feats), size=(h, w), mode='bilinear', align_corners=True) for stage in self.stages] + [feats]
        bottle = self.bottleneck(torch.cat(priors, 1))
        return bottle

class Decoder_Module(nn.Module):

    def __init__(self, num_classes):
        super(Decoder_Module, self).__init__()
        self.conv1 = nn.Sequential(
            nn.Conv2d(512, 256, kernel_size=1, padding=0, dilation=1, bias=False),
            InPlaceABNSync(256)
            )
        self.conv2 = nn.Sequential(
            nn.Conv2d(256, 48, kernel_size=1, stride=1, padding=0, dilation=1, bias=False),
            InPlaceABNSync(48)
            )
        self.conv3 = nn.Sequential(
            nn.Conv2d(768, 512, kernel_size=1, padding=0, dilation=1, bias=False),
            InPlaceABNSync(512)
            )
        #  self.conv3 = nn.Sequential(
                #  nn.Conv2d(304, 256, kernel_size=3, padding=1, dilation=1, bias=False),
                #  InPlaceABNSync(256),
                #  nn.Conv2d(256, 512, kernel_size=3, padding=1, dilation=1, bias=False),
                #  InPlaceABNSync(512))
        self.conv4 = nn.Conv2d(512, num_classes, kernel_size=1, padding=0, dilation=1, bias=True)

    def forward(self, xt, xl):
        _, _, h, w = xl.size()

        xt = F.interpolate(xt, size=(h, w), mode='bilinear')
        x = torch.cat([xt, xl], dim=1)
        x = self.conv3(x)
        seg = self.conv4(x)
        return seg, x
#
# class SpatialAttention(nn.Module):
#     def __init__(self, kernel_size=7):
#         super(SpatialAttention, self).__init__()
#
#         assert kernel_size in (3, 7), 'kernel size must be 3 or 7'
#         padding = 3 if kernel_size == 7 else 1
#
#         self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
#         self.sigmoid = nn.Sigmoid()
#         #self.register_buffer()
#
#     def forward(self, x):
#         avg_out = torch.mean(x, dim=1, keepdim=True)
#         max_out, _ = torch.max(x, dim=1, keepdim=True)
#         x = torch.cat([avg_out, max_out], dim=1)
#         x = self.conv1(x)
#         return self.sigmoid(x)
#
# class SoftAttn(nn.Module):#软注意力（32，16，256，128）=空间注意力的输出（32，1，256，128）乘上通道注意力（32,16,1,1）
#
#     def __init__(self, in_channels):
#         super(SoftAttn, self).__init__()
#         self.spatial_attn = SpatialAttention()
#         self.channel_attn = ChannelAttention(in_channels)
#         #self.conv = ConvBlock(in_channels, in_channels, 1)
#
#     def forward(self, x):#x.shape(32,16,256,128)
#         y_spatial = self.spatial_attn(x)#32,1,256,128
#         y_channel = self.channel_attn(x)#32,16,1,1
#         y = y_spatial * y_channel#32,16,256,128
#         y = F.sigmoid(y)
#         return y#torch.Size([32, 16, 256, 128])
class ResNet(nn.Module):
    def __init__(self, block, layers, num_classes, npoints):
        self.inplanes = 128
        super(ResNet, self).__init__()

        self.conv1 = conv3x3(3, 64, stride=2)
        self.bn1 = BatchNorm2d(64)
        self.relu1 = nn.ReLU(inplace=False)
        self.conv2 = conv3x3(64, 64)
        self.bn2 = BatchNorm2d(64)
        self.relu2 = nn.ReLU(inplace=False)
        self.conv3 = conv3x3(64, 128)
        self.bn3 = BatchNorm2d(128)
        self.relu3 = nn.ReLU(inplace=False)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=2, multi_grid=(1,1,1))
        self.layer5 = ASPPModule(features=2048, inner_features=256, out_features=512)

        self.module_list = []
        self.edge_layer = Edge_Module()
        #self.pose_layer = get_pose_net(npoints, is_train=True)
        self.pose_layer = get_hrnet_pose(cfg, is_train=True)
        self.layer6 = Decoder_Module(num_classes)

        self.module_list.append(self.edge_layer)
        self.module_list.append(self.pose_layer)

        self.out_layer = nn.Conv2d(512, num_classes, kernel_size=1, padding=0, bias=True)
        #
        # self.conv_fuse = nn.Sequential(
        #                               nn.Conv2d(1536, 512, kernel_size=1, padding=0, bias=False),
        #                               InPlaceABNSync(512)
        #                               )
        self.conv_fuse = nn.Sequential(
                                        nn.Conv2d(1040, 512, kernel_size=1, padding=0, bias=False),
                                         InPlaceABNSync(512)
        )#1552
        # #self.nl_SEP = NONLocalBlock2D(in_channels=512, sub_sample=True)
        self.nl_SEP = NONLocalBlock2D(in_channels=512)
        #self.nl_SEP = ImageDepthNet(3)

    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, multi_grid=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                BatchNorm2d(planes * block.expansion, affine=affine_par))

        layers = []
        generate_multi_grid = lambda index, grids: grids[index%len(grids)] if isinstance(grids, tuple) else 1
        layers.append(block(self.inplanes, planes, stride,dilation=dilation, downsample=downsample, multi_grid=generate_multi_grid(0, multi_grid)))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes, dilation=dilation, multi_grid=generate_multi_grid(i, multi_grid)))

        return nn.Sequential(*layers)

    # def Spatial_forward_features(self, x):
    #     b, _, f, p = x.shape  ##### b is batch size, f is number of frames, p is number of joints
    #     x = rearrange(x, 'b c f p  -> (b f) p  c', )
    #
    #     x = self.Spatial_patch_to_embedding(x)#(256,128,32)
    #     x += self.Spatial_pos_embed#self.Spatial_pos_embed(1,16,32)
    #     x = self.pos_drop(x)
    #
    #     for blk in self.Spatial_blocks:
    #         x = blk(x)
    #
    #     x = self.Spatial_norm(x)
    #     x = rearrange(x, '(b f) w c -> b f (w c)', f=f)
    #     return x



    def forward(self, x1):
        x = self.relu1(self.bn1(self.conv1(x1)))
        x = self.relu2(self.bn2(self.conv2(x)))
        x = self.relu3(self.bn3(self.conv3(x)))
        x = self.maxpool(x)
        x2 = self.layer1(x)
        x3 = self.layer2(x2)
        x4 = self.layer3(x3)
        x5 = self.layer4(x4)#(bz,2048,32,32)
        x = self.layer5(x5)
        edge1, edge_fea = self.edge_layer(x2, x3, x4)#edge1:tensor(1,2,128,128) edge_fea:tensor(1,512,128,128)
        pose1, pose_fea = self.pose_layer(x1)#pose1:tensor(1,16,128,128) pose_fea:tensor(1,512,128,128)
        #pose1 = self.Spatial_forward_features(pose1)
        seg1, x = self.layer6(x, x2)#seg1:tensor(1,20,128,128) x:tensor(1,512,128,128)

        h, w = x.size(2), x.size(3)
        #SEP_fea = self.conv_fuse(torch.cat((x, edge_fea, pose_fea), dim=1))  # tensor:(1,512,64,64)
        SEP_fea = self.conv_fuse(torch.cat((x, pose_fea, pose1), dim=1))#tensor:(1,512,64,64)
        #SEP_seg_correlation = self.nl_SEP(x, SEP_fea)#tensor(1,512,64,64)
        SEP_seg_correlation = self.nl_SEP(SEP_fea, x)  #tensor(1,512,64,64)
        seg2 = self.out_layer(SEP_seg_correlation)#tensor(1,20,64,64)
        return [[seg1, seg2], [edge1], [pose1]]


def CorrPM_Model(num_classes=21, npoints=16):
    model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes, npoints)
    return model

# skeletonFeats = [[] for _ in range(10000)]
# for i, (kpts) in enumerate(pose_fea):
#     skeletonFeats[i] = np.zeros((10000, 55, 64, 64), dtype=np.float32)
#     for j, kpt in enumerate(kpts):
#         if self.training:
#             mAug, _ = translib.get_aug_matrix(64, 64,
#                                               64, 64,
#                                               angle_range=(-30, 30),
#                                               scale_range=(0.8, 1.2),
#                                               trans_range=(-0.1, 0.1))
#             m3 = mAug
#         skeletonFeats[i][j] = genSkeletons(translib.warpAffineKpts([kpt], m3),
#                                                 64, 64,
#                                                 stride=1, sigma=3, threshold=1,
#                                                 visdiff=True).transpose(2, 0, 1)
#         skeletons = np.vstack(self.skeletonFeats)  # (1,55,64,64)---->(2,55,64,64)
#         skeletons = torch.from_numpy(skeletons).float().cuda(0)
#         pose_fea = torch.cat((pose_fea, skeletons), 1)
